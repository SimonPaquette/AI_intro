{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSI 4106 - A2\n",
    "\n",
    "Simon Paquette\n",
    "\n",
    "300044038\n",
    "\n",
    "spaqu044@uottawa.ca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 5064), started 7:29:21 ago. (Use '!kill 5064' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b67a3b152b8c9b1d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b67a3b152b8c9b1d\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as python_random\n",
    "import shutil\n",
    "import os\n",
    "import datetime\n",
    "from PIL import Image\n",
    "from multiprocessing import cpu_count\n",
    "from joblib import Parallel, delayed\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, mean_absolute_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# for reproducibility purposes\n",
    "SEED = 123\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# load tensorboard extension\n",
    "%reload_ext tensorboard\n",
    "# specify the log directory where the tensorboard logs will be written\n",
    "%tensorboard --logdir logs\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the relevant datasets (15/100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (15026, 4)\n",
      "Val shape: (3757, 4)\n",
      "Test shape: (4696, 4)\n",
      "Index(['age', 'ethnicity', 'gender', 'img_name'], dtype='object')\n",
      "   age  ethnicity  gender                        img_name\n",
      "0   57          0       0  20170120222949154.jpg.chip.jpg\n",
      "1   42          0       1  20170116221807908.jpg.chip.jpg\n",
      "2   57          0       0  20170104212808228.jpg.chip.jpg\n",
      "3   25          0       0  20170117192955499.jpg.chip.jpg\n",
      "4   46          3       0  20170119201157876.jpg.chip.jpg\n",
      "Train gender:\n",
      " 0    52.335951\n",
      "1    47.664049\n",
      "Name: gender, dtype: float64\n",
      "Val gender:\n",
      " 0    52.302369\n",
      "1    47.697631\n",
      "Name: gender, dtype: float64\n",
      "Test gender:\n",
      " 0    52.29983\n",
      "1    47.70017\n",
      "Name: gender, dtype: float64\n",
      "Train ethnicity:\n",
      " 0    42.406495\n",
      "1    19.093571\n",
      "3    16.797551\n",
      "2    14.548117\n",
      "4     7.154266\n",
      "Name: ethnicity, dtype: float64\n",
      "Val ethnicity:\n",
      " 0    42.400852\n",
      "1    19.084376\n",
      "3    16.821932\n",
      "2    14.559489\n",
      "4     7.133351\n",
      "Name: ethnicity, dtype: float64\n",
      "Test ethnicity:\n",
      " 0    42.397785\n",
      "1    19.080068\n",
      "3    16.822828\n",
      "2    14.544293\n",
      "4     7.155026\n",
      "Name: ethnicity, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the datasets using the csv files train, val and test\n",
    "# (3)\n",
    "train_path_csv = Path(\"data/train.csv\")\n",
    "val_path_csv = Path(\"data/val.csv\")\n",
    "test_path_csv = Path(\"data/test.csv\")\n",
    "\n",
    "train_df = pd.read_csv(train_path_csv)\n",
    "val_df = pd.read_csv(val_path_csv)\n",
    "test_df = pd.read_csv(test_path_csv)\n",
    "\n",
    "# print the shapes of the dataframes\n",
    "# (3)\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Val shape:\", val_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "\n",
    "# print the column names from either one of the dataframes\n",
    "# (1)\n",
    "print(train_df.columns)\n",
    "print(train_df.head())  # NOTE: added for me to visualize\n",
    "\n",
    "# print the proportional distribution of gender in all three datasets(i.e., number of male and female)\n",
    "# (3)\n",
    "print(\"Train gender:\\n\", train_df[\"gender\"].value_counts(normalize=True) * 100)\n",
    "print(\"Val gender:\\n\", val_df[\"gender\"].value_counts(normalize=True) * 100)\n",
    "print(\"Test gender:\\n\", test_df[\"gender\"].value_counts(normalize=True) * 100)\n",
    "\n",
    "# print the proportional distribution of ethnicity in all three datasets\n",
    "# (3)\n",
    "print(\"Train ethnicity:\\n\", train_df[\"ethnicity\"].value_counts(normalize=True) * 100)\n",
    "print(\"Val ethnicity:\\n\", val_df[\"ethnicity\"].value_counts(normalize=True) * 100)\n",
    "print(\"Test ethnicity:\\n\", test_df[\"ethnicity\"].value_counts(normalize=True) * 100)\n",
    "\n",
    "# NOTE: HERE, the ethnicity is imbalanced, i will use class weight later on\n",
    "\n",
    "# plot the age distribution from the training dataset where the x-axis plots the age and the y-axis depicts the count of individuals within each age group. For example, individuals with age=1 are:\n",
    "# (2)\n",
    "age_df = train_df[\"age\"]\n",
    "age_df.hist(bins=50, figsize=(20, 15))\n",
    "plt.xlabel(\"age\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the ImageDataGenerators (22/100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15026 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "Found 3757 validated image filenames.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ImageDataGenerator is an iterator.\n",
    "\n",
    "# specify the batch size hyperparameter. You can experiment with different batch sizes\n",
    "batch_size = 32\n",
    "HEIGHT = 48\n",
    "WIDTH = 48\n",
    "SIZE = (HEIGHT, WIDTH)\n",
    "\n",
    "# create the ImageDataGenerator with rescaling that will generate batched tensors representing images with real-time data augmentation\n",
    "# use at least two of the augmentation strategies. For example, fill_mode='nearest'\n",
    "# please refer: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n",
    "# (3)\n",
    "train_img_gen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    fill_mode=\"nearest\",\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    ")\n",
    "\n",
    "# use the method \"flow_from_dataframe\" from the \"ImageDataGenerator\" instance to link the image folder and the dataframe.\n",
    "# also include the, batch size, image size and the seed.\n",
    "# make sure to include the following arguments\n",
    "# color_mode='grayscale', class_mode='multi_output'\n",
    "# please refer: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n",
    "# (5)\n",
    "train_generator = train_img_gen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    directory=\"data/images/train\",\n",
    "    x_col=\"img_name\",\n",
    "    y_col=[\"age\", \"ethnicity\", \"gender\"],\n",
    "    target_size=SIZE,\n",
    "    color_mode=\"grayscale\",\n",
    "    class_mode=\"multi_output\",\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "\n",
    "# similarly, create an ImageDataGenerator for the validation dataset and make sure not to use any of th eaugmentation strategies except rescaling the image\n",
    "# (2)\n",
    "val_img_gen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    ")\n",
    "\n",
    "# use the method \"flow_from_dataframe\" from the \"ImageDataGenerator\" instance with the same arguments as above\n",
    "# make sure to specify the following arguments:\n",
    "# class_mode='multi_output', color_mode='grayscale', shuffle=False\n",
    "# (5)\n",
    "# NOTE: i assume here its for test dataset ??\n",
    "test_generator = val_img_gen.flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    directory=\"data/images/test\",\n",
    "    x_col=\"img_name\",\n",
    "    y_col=[\"age\", \"ethnicity\", \"gender\"],\n",
    "    target_size=SIZE,\n",
    "    color_mode=\"grayscale\",\n",
    "    class_mode=\"multi_output\",\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# use the method \"flow_from_dataframe\" from the val_img_gen instance to link the test dataframe and the test data folder\n",
    "# In addition, make sure to specify the following arguments\n",
    "# class_mode='multi_output', color_mode='grayscale', shuffle=False\n",
    "# (5)\n",
    "# NOTE: i assume here its for validation dataset ??\n",
    "val_generator = val_img_gen.flow_from_dataframe(\n",
    "    dataframe=val_df,\n",
    "    directory=\"data/images/val\",\n",
    "    x_col=\"img_name\",\n",
    "    y_col=[\"age\", \"ethnicity\", \"gender\"],\n",
    "    target_size=SIZE,\n",
    "    color_mode=\"grayscale\",\n",
    "    class_mode=\"multi_output\",\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "\n",
    "# enumerate through the validation data generator created above and plot first grayscale image\n",
    "# (2)\n",
    "for i, element in enumerate(val_generator):\n",
    "    features, labels = element\n",
    "    image = features[1:2]\n",
    "    image = np.resize(image, SIZE)\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "    plt.plot()\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the model (44/100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 48, 48, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " IN_SHARED (Conv2D)             (None, 48, 48, 64)   1664        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 48, 48, 64)   102464      ['IN_SHARED[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 24, 24, 64)   0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 24, 24, 128)  73856       ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 24, 24, 128)  147584      ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 12, 12, 128)  0          ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 12, 12, 256)  295168      ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 12, 12, 256)  590080      ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 6, 6, 256)   0           ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " OUT_SHARED_FLATTEN (Flatten)   (None, 9216)         0           ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " IN_AGE (Dense)                 (None, 256)          2359552     ['OUT_SHARED_FLATTEN[0][0]']     \n",
      "                                                                                                  \n",
      " IN_ETHNICITY (Dense)           (None, 256)          2359552     ['OUT_SHARED_FLATTEN[0][0]']     \n",
      "                                                                                                  \n",
      " IN_GENDER (Dense)              (None, 128)          1179776     ['OUT_SHARED_FLATTEN[0][0]']     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          32896       ['IN_AGE[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 128)          32896       ['IN_ETHNICITY[0][0]']           \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 128)          0           ['IN_GENDER[0][0]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64)           8256        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 64)           8256        ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 64)           8256        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " OUT_AGE_LINEAR (Dense)         (None, 1)            65          ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " OUT_ETHNICITY_SOFTMAX (Dense)  (None, 5)            325         ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " OUT_GENDER_SIGMOID (Dense)     (None, 1)            65          ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7,200,711\n",
      "Trainable params: 7,200,711\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Class weights: {'OUT_AGE_LINEAR': None, 'OUT_ETHNICITY_SOFTMAX': {0: 0.4716258631512869, 1: 1.0474729871035204, 2: 1.3747483989021043, 3: 1.190649762282092, 4: 2.79553488372093}, 'OUT_GENDER_SIGMOID': None}\n",
      "Epoch 1/50\n",
      "470/469 [==============================] - ETA: 0s - loss: 1.6100 - OUT_AGE_LINEAR_loss: 491.6559 - OUT_ETHNICITY_SOFTMAX_loss: 1.5341 - OUT_GENDER_SIGMOID_loss: 0.7026 - OUT_AGE_LINEAR_MAE_AGE: 16.9654 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.4199 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.5246\n",
      "Epoch 1: val_loss improved from inf to 1.44350, saving model to models\\1652233544\n",
      "INFO:tensorflow:Assets written to: models\\1652233544\\assets\n",
      "469/469 [==============================] - 82s 152ms/step - loss: 1.6100 - OUT_AGE_LINEAR_loss: 491.6559 - OUT_ETHNICITY_SOFTMAX_loss: 1.5341 - OUT_GENDER_SIGMOID_loss: 0.7026 - OUT_AGE_LINEAR_MAE_AGE: 16.9654 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.4199 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.5246 - val_loss: 1.4435 - val_OUT_AGE_LINEAR_loss: 390.8963 - val_OUT_ETHNICITY_SOFTMAX_loss: 1.4348 - val_OUT_GENDER_SIGMOID_loss: 0.6704 - val_OUT_AGE_LINEAR_MAE_AGE: 15.7103 - val_OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.4240 - val_OUT_GENDER_SIGMOID_ACC_GENDER: 0.5869 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "470/469 [==============================] - ETA: 0s - loss: 1.4205 - OUT_AGE_LINEAR_loss: 395.4437 - OUT_ETHNICITY_SOFTMAX_loss: 1.4019 - OUT_GENDER_SIGMOID_loss: 0.6481 - OUT_AGE_LINEAR_MAE_AGE: 15.2076 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.4483 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.6303\n",
      "Epoch 2: val_loss improved from 1.44350 to 1.31077, saving model to models\\1652233544\n",
      "INFO:tensorflow:Assets written to: models\\1652233544\\assets\n",
      "469/469 [==============================] - 29s 62ms/step - loss: 1.4205 - OUT_AGE_LINEAR_loss: 395.4437 - OUT_ETHNICITY_SOFTMAX_loss: 1.4019 - OUT_GENDER_SIGMOID_loss: 0.6481 - OUT_AGE_LINEAR_MAE_AGE: 15.2076 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.4483 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.6303 - val_loss: 1.3108 - val_OUT_AGE_LINEAR_loss: 342.7166 - val_OUT_ETHNICITY_SOFTMAX_loss: 1.3383 - val_OUT_GENDER_SIGMOID_loss: 0.5978 - val_OUT_AGE_LINEAR_MAE_AGE: 15.2231 - val_OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.4621 - val_OUT_GENDER_SIGMOID_ACC_GENDER: 0.6894 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "470/469 [==============================] - ETA: 0s - loss: 1.2999 - OUT_AGE_LINEAR_loss: 329.2661 - OUT_ETHNICITY_SOFTMAX_loss: 1.3338 - OUT_GENDER_SIGMOID_loss: 0.6075 - OUT_AGE_LINEAR_MAE_AGE: 13.8529 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.4755 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.6722\n",
      "Epoch 3: val_loss improved from 1.31077 to 1.23942, saving model to models\\1652233544\n",
      "INFO:tensorflow:Assets written to: models\\1652233544\\assets\n",
      "469/469 [==============================] - 30s 64ms/step - loss: 1.2999 - OUT_AGE_LINEAR_loss: 329.2661 - OUT_ETHNICITY_SOFTMAX_loss: 1.3338 - OUT_GENDER_SIGMOID_loss: 0.6075 - OUT_AGE_LINEAR_MAE_AGE: 13.8529 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.4755 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.6722 - val_loss: 1.2394 - val_OUT_AGE_LINEAR_loss: 342.3974 - val_OUT_ETHNICITY_SOFTMAX_loss: 1.2619 - val_OUT_GENDER_SIGMOID_loss: 0.5321 - val_OUT_AGE_LINEAR_MAE_AGE: 13.0528 - val_OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.5105 - val_OUT_GENDER_SIGMOID_ACC_GENDER: 0.7312 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "470/469 [==============================] - ETA: 0s - loss: 1.2336 - OUT_AGE_LINEAR_loss: 301.6145 - OUT_ETHNICITY_SOFTMAX_loss: 1.2880 - OUT_GENDER_SIGMOID_loss: 0.5760 - OUT_AGE_LINEAR_MAE_AGE: 13.0819 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.4965 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.6965\n",
      "Epoch 4: val_loss improved from 1.23942 to 1.10638, saving model to models\\1652233544\n",
      "INFO:tensorflow:Assets written to: models\\1652233544\\assets\n",
      "469/469 [==============================] - 31s 65ms/step - loss: 1.2336 - OUT_AGE_LINEAR_loss: 301.6145 - OUT_ETHNICITY_SOFTMAX_loss: 1.2880 - OUT_GENDER_SIGMOID_loss: 0.5760 - OUT_AGE_LINEAR_MAE_AGE: 13.0819 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.4965 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.6965 - val_loss: 1.1064 - val_OUT_AGE_LINEAR_loss: 240.2530 - val_OUT_ETHNICITY_SOFTMAX_loss: 1.2041 - val_OUT_GENDER_SIGMOID_loss: 0.5281 - val_OUT_AGE_LINEAR_MAE_AGE: 12.0039 - val_OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.5355 - val_OUT_GENDER_SIGMOID_ACC_GENDER: 0.7306 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "470/469 [==============================] - ETA: 0s - loss: 1.1827 - OUT_AGE_LINEAR_loss: 280.2025 - OUT_ETHNICITY_SOFTMAX_loss: 1.2587 - OUT_GENDER_SIGMOID_loss: 0.5463 - OUT_AGE_LINEAR_MAE_AGE: 12.4948 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.5117 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.7245\n",
      "Epoch 5: val_loss improved from 1.10638 to 1.03721, saving model to models\\1652233544\n",
      "INFO:tensorflow:Assets written to: models\\1652233544\\assets\n",
      "469/469 [==============================] - 31s 67ms/step - loss: 1.1827 - OUT_AGE_LINEAR_loss: 280.2025 - OUT_ETHNICITY_SOFTMAX_loss: 1.2587 - OUT_GENDER_SIGMOID_loss: 0.5463 - OUT_AGE_LINEAR_MAE_AGE: 12.4948 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.5117 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.7245 - val_loss: 1.0372 - val_OUT_AGE_LINEAR_loss: 214.1912 - val_OUT_ETHNICITY_SOFTMAX_loss: 1.1394 - val_OUT_GENDER_SIGMOID_loss: 0.5066 - val_OUT_AGE_LINEAR_MAE_AGE: 11.2054 - val_OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.5648 - val_OUT_GENDER_SIGMOID_ACC_GENDER: 0.7567 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "470/469 [==============================] - ETA: 0s - loss: 1.1401 - OUT_AGE_LINEAR_loss: 266.4574 - OUT_ETHNICITY_SOFTMAX_loss: 1.2263 - OUT_GENDER_SIGMOID_loss: 0.5209 - OUT_AGE_LINEAR_MAE_AGE: 12.1280 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.5293 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.7410\n",
      "Epoch 6: val_loss did not improve from 1.03721\n",
      "469/469 [==============================] - 29s 61ms/step - loss: 1.1401 - OUT_AGE_LINEAR_loss: 266.4574 - OUT_ETHNICITY_SOFTMAX_loss: 1.2263 - OUT_GENDER_SIGMOID_loss: 0.5209 - OUT_AGE_LINEAR_MAE_AGE: 12.1280 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.5293 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.7410 - val_loss: 1.0752 - val_OUT_AGE_LINEAR_loss: 244.4631 - val_OUT_ETHNICITY_SOFTMAX_loss: 1.1909 - val_OUT_GENDER_SIGMOID_loss: 0.4705 - val_OUT_AGE_LINEAR_MAE_AGE: 12.4480 - val_OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.5398 - val_OUT_GENDER_SIGMOID_ACC_GENDER: 0.7695 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "470/469 [==============================] - ETA: 0s - loss: 1.0882 - OUT_AGE_LINEAR_loss: 252.6634 - OUT_ETHNICITY_SOFTMAX_loss: 1.1774 - OUT_GENDER_SIGMOID_loss: 0.4935 - OUT_AGE_LINEAR_MAE_AGE: 11.7884 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.5523 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.7620\n",
      "Epoch 7: val_loss improved from 1.03721 to 0.92179, saving model to models\\1652233544\n",
      "INFO:tensorflow:Assets written to: models\\1652233544\\assets\n",
      "469/469 [==============================] - 31s 66ms/step - loss: 1.0882 - OUT_AGE_LINEAR_loss: 252.6634 - OUT_ETHNICITY_SOFTMAX_loss: 1.1774 - OUT_GENDER_SIGMOID_loss: 0.4935 - OUT_AGE_LINEAR_MAE_AGE: 11.7884 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.5523 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.7620 - val_loss: 0.9218 - val_OUT_AGE_LINEAR_loss: 195.4099 - val_OUT_ETHNICITY_SOFTMAX_loss: 1.0297 - val_OUT_GENDER_SIGMOID_loss: 0.4231 - val_OUT_AGE_LINEAR_MAE_AGE: 10.4885 - val_OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.6186 - val_OUT_GENDER_SIGMOID_ACC_GENDER: 0.7982 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "470/469 [==============================] - ETA: 0s - loss: 1.0454 - OUT_AGE_LINEAR_loss: 247.1535 - OUT_ETHNICITY_SOFTMAX_loss: 1.1301 - OUT_GENDER_SIGMOID_loss: 0.4664 - OUT_AGE_LINEAR_MAE_AGE: 11.6182 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.5722 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.7786\n",
      "Epoch 8: val_loss improved from 0.92179 to 0.89637, saving model to models\\1652233544\n",
      "INFO:tensorflow:Assets written to: models\\1652233544\\assets\n",
      "469/469 [==============================] - 30s 65ms/step - loss: 1.0454 - OUT_AGE_LINEAR_loss: 247.1535 - OUT_ETHNICITY_SOFTMAX_loss: 1.1301 - OUT_GENDER_SIGMOID_loss: 0.4664 - OUT_AGE_LINEAR_MAE_AGE: 11.6182 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.5722 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.7786 - val_loss: 0.8964 - val_OUT_AGE_LINEAR_loss: 181.6908 - val_OUT_ETHNICITY_SOFTMAX_loss: 1.0387 - val_OUT_GENDER_SIGMOID_loss: 0.3907 - val_OUT_AGE_LINEAR_MAE_AGE: 9.8990 - val_OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.6066 - val_OUT_GENDER_SIGMOID_ACC_GENDER: 0.8171 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "470/469 [==============================] - ETA: 0s - loss: 0.9960 - OUT_AGE_LINEAR_loss: 233.1219 - OUT_ETHNICITY_SOFTMAX_loss: 1.0763 - OUT_GENDER_SIGMOID_loss: 0.4494 - OUT_AGE_LINEAR_MAE_AGE: 11.2656 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.5956 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.7881\n",
      "Epoch 9: val_loss improved from 0.89637 to 0.89304, saving model to models\\1652233544\n",
      "INFO:tensorflow:Assets written to: models\\1652233544\\assets\n",
      "469/469 [==============================] - 31s 66ms/step - loss: 0.9960 - OUT_AGE_LINEAR_loss: 233.1219 - OUT_ETHNICITY_SOFTMAX_loss: 1.0763 - OUT_GENDER_SIGMOID_loss: 0.4494 - OUT_AGE_LINEAR_MAE_AGE: 11.2656 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.5956 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.7881 - val_loss: 0.8930 - val_OUT_AGE_LINEAR_loss: 166.0081 - val_OUT_ETHNICITY_SOFTMAX_loss: 1.0459 - val_OUT_GENDER_SIGMOID_loss: 0.4082 - val_OUT_AGE_LINEAR_MAE_AGE: 9.3126 - val_OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.6295 - val_OUT_GENDER_SIGMOID_ACC_GENDER: 0.8145 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "470/469 [==============================] - ETA: 0s - loss: 0.9649 - OUT_AGE_LINEAR_loss: 225.5149 - OUT_ETHNICITY_SOFTMAX_loss: 1.0482 - OUT_GENDER_SIGMOID_loss: 0.4306 - OUT_AGE_LINEAR_MAE_AGE: 11.0290 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.6092 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8011\n",
      "Epoch 10: val_loss did not improve from 0.89304\n",
      "469/469 [==============================] - 29s 62ms/step - loss: 0.9649 - OUT_AGE_LINEAR_loss: 225.5149 - OUT_ETHNICITY_SOFTMAX_loss: 1.0482 - OUT_GENDER_SIGMOID_loss: 0.4306 - OUT_AGE_LINEAR_MAE_AGE: 11.0290 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.6092 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8011 - val_loss: 0.9970 - val_OUT_AGE_LINEAR_loss: 310.4159 - val_OUT_ETHNICITY_SOFTMAX_loss: 0.9633 - val_OUT_GENDER_SIGMOID_loss: 0.4097 - val_OUT_AGE_LINEAR_MAE_AGE: 12.6193 - val_OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.6670 - val_OUT_GENDER_SIGMOID_ACC_GENDER: 0.7964 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "470/469 [==============================] - ETA: 0s - loss: 0.9428 - OUT_AGE_LINEAR_loss: 218.6836 - OUT_ETHNICITY_SOFTMAX_loss: 1.0244 - OUT_GENDER_SIGMOID_loss: 0.4238 - OUT_AGE_LINEAR_MAE_AGE: 10.8216 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.6177 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8061\n",
      "Epoch 11: val_loss improved from 0.89304 to 0.84477, saving model to models\\1652233544\n",
      "INFO:tensorflow:Assets written to: models\\1652233544\\assets\n",
      "469/469 [==============================] - 32s 68ms/step - loss: 0.9428 - OUT_AGE_LINEAR_loss: 218.6836 - OUT_ETHNICITY_SOFTMAX_loss: 1.0244 - OUT_GENDER_SIGMOID_loss: 0.4238 - OUT_AGE_LINEAR_MAE_AGE: 10.8216 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.6177 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8061 - val_loss: 0.8448 - val_OUT_AGE_LINEAR_loss: 212.2165 - val_OUT_ETHNICITY_SOFTMAX_loss: 0.8882 - val_OUT_GENDER_SIGMOID_loss: 0.3770 - val_OUT_AGE_LINEAR_MAE_AGE: 10.0443 - val_OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.6785 - val_OUT_GENDER_SIGMOID_ACC_GENDER: 0.8246 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "470/469 [==============================] - ETA: 0s - loss: 0.9137 - OUT_AGE_LINEAR_loss: 208.1235 - OUT_ETHNICITY_SOFTMAX_loss: 0.9968 - OUT_GENDER_SIGMOID_loss: 0.4144 - OUT_AGE_LINEAR_MAE_AGE: 10.5195 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.6336 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8072\n",
      "Epoch 12: val_loss improved from 0.84477 to 0.79791, saving model to models\\1652233544\n",
      "INFO:tensorflow:Assets written to: models\\1652233544\\assets\n",
      "469/469 [==============================] - 33s 70ms/step - loss: 0.9137 - OUT_AGE_LINEAR_loss: 208.1235 - OUT_ETHNICITY_SOFTMAX_loss: 0.9968 - OUT_GENDER_SIGMOID_loss: 0.4144 - OUT_AGE_LINEAR_MAE_AGE: 10.5195 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.6336 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8072 - val_loss: 0.7979 - val_OUT_AGE_LINEAR_loss: 156.7837 - val_OUT_ETHNICITY_SOFTMAX_loss: 0.9356 - val_OUT_GENDER_SIGMOID_loss: 0.3466 - val_OUT_AGE_LINEAR_MAE_AGE: 9.4241 - val_OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.6715 - val_OUT_GENDER_SIGMOID_ACC_GENDER: 0.8475 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "470/469 [==============================] - ETA: 0s - loss: 0.8959 - OUT_AGE_LINEAR_loss: 198.5488 - OUT_ETHNICITY_SOFTMAX_loss: 0.9912 - OUT_GENDER_SIGMOID_loss: 0.4036 - OUT_AGE_LINEAR_MAE_AGE: 10.2709 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.6408 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8182\n",
      "Epoch 13: val_loss improved from 0.79791 to 0.74184, saving model to models\\1652233544\n",
      "INFO:tensorflow:Assets written to: models\\1652233544\\assets\n",
      "469/469 [==============================] - 31s 66ms/step - loss: 0.8959 - OUT_AGE_LINEAR_loss: 198.5488 - OUT_ETHNICITY_SOFTMAX_loss: 0.9912 - OUT_GENDER_SIGMOID_loss: 0.4036 - OUT_AGE_LINEAR_MAE_AGE: 10.2709 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.6408 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8182 - val_loss: 0.7418 - val_OUT_AGE_LINEAR_loss: 157.0911 - val_OUT_ETHNICITY_SOFTMAX_loss: 0.8280 - val_OUT_GENDER_SIGMOID_loss: 0.3415 - val_OUT_AGE_LINEAR_MAE_AGE: 9.4883 - val_OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7003 - val_OUT_GENDER_SIGMOID_ACC_GENDER: 0.8440 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "470/469 [==============================] - ETA: 0s - loss: 0.8767 - OUT_AGE_LINEAR_loss: 197.0874 - OUT_ETHNICITY_SOFTMAX_loss: 0.9700 - OUT_GENDER_SIGMOID_loss: 0.3893 - OUT_AGE_LINEAR_MAE_AGE: 10.1996 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.6451 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8252\n",
      "Epoch 14: val_loss did not improve from 0.74184\n",
      "469/469 [==============================] - 29s 61ms/step - loss: 0.8767 - OUT_AGE_LINEAR_loss: 197.0874 - OUT_ETHNICITY_SOFTMAX_loss: 0.9700 - OUT_GENDER_SIGMOID_loss: 0.3893 - OUT_AGE_LINEAR_MAE_AGE: 10.1996 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.6451 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8252 - val_loss: 0.8654 - val_OUT_AGE_LINEAR_loss: 143.3237 - val_OUT_ETHNICITY_SOFTMAX_loss: 1.1187 - val_OUT_GENDER_SIGMOID_loss: 0.3255 - val_OUT_AGE_LINEAR_MAE_AGE: 8.5956 - val_OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.6625 - val_OUT_GENDER_SIGMOID_ACC_GENDER: 0.8531 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "470/469 [==============================] - ETA: 0s - loss: 0.8701 - OUT_AGE_LINEAR_loss: 194.8715 - OUT_ETHNICITY_SOFTMAX_loss: 0.9608 - OUT_GENDER_SIGMOID_loss: 0.3896 - OUT_AGE_LINEAR_MAE_AGE: 10.1640 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.6455 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8284\n",
      "Epoch 15: val_loss did not improve from 0.74184\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "469/469 [==============================] - 31s 66ms/step - loss: 0.8701 - OUT_AGE_LINEAR_loss: 194.8715 - OUT_ETHNICITY_SOFTMAX_loss: 0.9608 - OUT_GENDER_SIGMOID_loss: 0.3896 - OUT_AGE_LINEAR_MAE_AGE: 10.1640 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.6455 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8284 - val_loss: 0.7990 - val_OUT_AGE_LINEAR_loss: 143.5907 - val_OUT_ETHNICITY_SOFTMAX_loss: 0.9833 - val_OUT_GENDER_SIGMOID_loss: 0.3276 - val_OUT_AGE_LINEAR_MAE_AGE: 8.6923 - val_OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.6460 - val_OUT_GENDER_SIGMOID_ACC_GENDER: 0.8509 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "470/469 [==============================] - ETA: 0s - loss: 0.7508 - OUT_AGE_LINEAR_loss: 153.9646 - OUT_ETHNICITY_SOFTMAX_loss: 0.8483 - OUT_GENDER_SIGMOID_loss: 0.3455 - OUT_AGE_LINEAR_MAE_AGE: 8.9729 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.6902 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8466\n",
      "Epoch 16: val_loss improved from 0.74184 to 0.66809, saving model to models\\1652233544\n",
      "INFO:tensorflow:Assets written to: models\\1652233544\\assets\n",
      "469/469 [==============================] - 32s 69ms/step - loss: 0.7508 - OUT_AGE_LINEAR_loss: 153.9646 - OUT_ETHNICITY_SOFTMAX_loss: 0.8483 - OUT_GENDER_SIGMOID_loss: 0.3455 - OUT_AGE_LINEAR_MAE_AGE: 8.9729 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.6902 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8466 - val_loss: 0.6681 - val_OUT_AGE_LINEAR_loss: 121.0078 - val_OUT_ETHNICITY_SOFTMAX_loss: 0.7912 - val_OUT_GENDER_SIGMOID_loss: 0.3030 - val_OUT_AGE_LINEAR_MAE_AGE: 7.9490 - val_OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7227 - val_OUT_GENDER_SIGMOID_ACC_GENDER: 0.8597 - lr: 1.0000e-04\n",
      "Epoch 17/50\n",
      "470/469 [==============================] - ETA: 0s - loss: 0.7333 - OUT_AGE_LINEAR_loss: 149.2546 - OUT_ETHNICITY_SOFTMAX_loss: 0.8316 - OUT_GENDER_SIGMOID_loss: 0.3364 - OUT_AGE_LINEAR_MAE_AGE: 8.8084 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.6973 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8481\n",
      "Epoch 17: val_loss improved from 0.66809 to 0.66404, saving model to models\\1652233544\n",
      "INFO:tensorflow:Assets written to: models\\1652233544\\assets\n",
      "469/469 [==============================] - 30s 65ms/step - loss: 0.7333 - OUT_AGE_LINEAR_loss: 149.2546 - OUT_ETHNICITY_SOFTMAX_loss: 0.8316 - OUT_GENDER_SIGMOID_loss: 0.3364 - OUT_AGE_LINEAR_MAE_AGE: 8.8084 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.6973 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8481 - val_loss: 0.6640 - val_OUT_AGE_LINEAR_loss: 118.7040 - val_OUT_ETHNICITY_SOFTMAX_loss: 0.7904 - val_OUT_GENDER_SIGMOID_loss: 0.3003 - val_OUT_AGE_LINEAR_MAE_AGE: 7.9269 - val_OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7227 - val_OUT_GENDER_SIGMOID_ACC_GENDER: 0.8635 - lr: 1.0000e-04\n",
      "Epoch 18/50\n",
      "470/469 [==============================] - ETA: 0s - loss: 0.7174 - OUT_AGE_LINEAR_loss: 146.0999 - OUT_ETHNICITY_SOFTMAX_loss: 0.8135 - OUT_GENDER_SIGMOID_loss: 0.3292 - OUT_AGE_LINEAR_MAE_AGE: 8.7263 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7059 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8517\n",
      "Epoch 18: val_loss improved from 0.66404 to 0.65798, saving model to models\\1652233544\n",
      "INFO:tensorflow:Assets written to: models\\1652233544\\assets\n",
      "469/469 [==============================] - 32s 68ms/step - loss: 0.7174 - OUT_AGE_LINEAR_loss: 146.0999 - OUT_ETHNICITY_SOFTMAX_loss: 0.8135 - OUT_GENDER_SIGMOID_loss: 0.3292 - OUT_AGE_LINEAR_MAE_AGE: 8.7263 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7059 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8517 - val_loss: 0.6580 - val_OUT_AGE_LINEAR_loss: 118.6793 - val_OUT_ETHNICITY_SOFTMAX_loss: 0.7835 - val_OUT_GENDER_SIGMOID_loss: 0.2951 - val_OUT_AGE_LINEAR_MAE_AGE: 7.8723 - val_OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7285 - val_OUT_GENDER_SIGMOID_ACC_GENDER: 0.8656 - lr: 1.0000e-04\n",
      "Epoch 19/50\n",
      "470/469 [==============================] - ETA: 0s - loss: 0.7128 - OUT_AGE_LINEAR_loss: 142.2853 - OUT_ETHNICITY_SOFTMAX_loss: 0.8122 - OUT_GENDER_SIGMOID_loss: 0.3290 - OUT_AGE_LINEAR_MAE_AGE: 8.5740 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7049 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8531\n",
      "Epoch 19: val_loss did not improve from 0.65798\n",
      "469/469 [==============================] - 29s 62ms/step - loss: 0.7128 - OUT_AGE_LINEAR_loss: 142.2853 - OUT_ETHNICITY_SOFTMAX_loss: 0.8122 - OUT_GENDER_SIGMOID_loss: 0.3290 - OUT_AGE_LINEAR_MAE_AGE: 8.5740 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7049 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8531 - val_loss: 0.6664 - val_OUT_AGE_LINEAR_loss: 124.9079 - val_OUT_ETHNICITY_SOFTMAX_loss: 0.7881 - val_OUT_GENDER_SIGMOID_loss: 0.2949 - val_OUT_AGE_LINEAR_MAE_AGE: 8.1433 - val_OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7282 - val_OUT_GENDER_SIGMOID_ACC_GENDER: 0.8696 - lr: 1.0000e-04\n",
      "Epoch 20/50\n",
      "470/469 [==============================] - ETA: 0s - loss: 0.7119 - OUT_AGE_LINEAR_loss: 142.8510 - OUT_ETHNICITY_SOFTMAX_loss: 0.8085 - OUT_GENDER_SIGMOID_loss: 0.3295 - OUT_AGE_LINEAR_MAE_AGE: 8.6235 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7083 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8574\n",
      "Epoch 20: val_loss improved from 0.65798 to 0.64671, saving model to models\\1652233544\n",
      "INFO:tensorflow:Assets written to: models\\1652233544\\assets\n",
      "469/469 [==============================] - 32s 69ms/step - loss: 0.7119 - OUT_AGE_LINEAR_loss: 142.8510 - OUT_ETHNICITY_SOFTMAX_loss: 0.8085 - OUT_GENDER_SIGMOID_loss: 0.3295 - OUT_AGE_LINEAR_MAE_AGE: 8.6235 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7083 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8574 - val_loss: 0.6467 - val_OUT_AGE_LINEAR_loss: 118.1954 - val_OUT_ETHNICITY_SOFTMAX_loss: 0.7680 - val_OUT_GENDER_SIGMOID_loss: 0.2890 - val_OUT_AGE_LINEAR_MAE_AGE: 7.9289 - val_OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7365 - val_OUT_GENDER_SIGMOID_ACC_GENDER: 0.8701 - lr: 1.0000e-04\n",
      "Epoch 21/50\n",
      "470/469 [==============================] - ETA: 0s - loss: 0.7010 - OUT_AGE_LINEAR_loss: 139.1203 - OUT_ETHNICITY_SOFTMAX_loss: 0.8004 - OUT_GENDER_SIGMOID_loss: 0.3234 - OUT_AGE_LINEAR_MAE_AGE: 8.5004 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7084 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8590\n",
      "Epoch 21: val_loss improved from 0.64671 to 0.64109, saving model to models\\1652233544\n",
      "INFO:tensorflow:Assets written to: models\\1652233544\\assets\n",
      "469/469 [==============================] - 32s 67ms/step - loss: 0.7010 - OUT_AGE_LINEAR_loss: 139.1203 - OUT_ETHNICITY_SOFTMAX_loss: 0.8004 - OUT_GENDER_SIGMOID_loss: 0.3234 - OUT_AGE_LINEAR_MAE_AGE: 8.5004 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7084 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8590 - val_loss: 0.6411 - val_OUT_AGE_LINEAR_loss: 113.6395 - val_OUT_ETHNICITY_SOFTMAX_loss: 0.7671 - val_OUT_GENDER_SIGMOID_loss: 0.2878 - val_OUT_AGE_LINEAR_MAE_AGE: 7.6866 - val_OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7384 - val_OUT_GENDER_SIGMOID_ACC_GENDER: 0.8677 - lr: 1.0000e-04\n",
      "Epoch 22/50\n",
      "470/469 [==============================] - ETA: 0s - loss: 0.6990 - OUT_AGE_LINEAR_loss: 137.0533 - OUT_ETHNICITY_SOFTMAX_loss: 0.8020 - OUT_GENDER_SIGMOID_loss: 0.3218 - OUT_AGE_LINEAR_MAE_AGE: 8.4267 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7093 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8582\n",
      "Epoch 22: val_loss did not improve from 0.64109\n",
      "469/469 [==============================] - 29s 62ms/step - loss: 0.6990 - OUT_AGE_LINEAR_loss: 137.0533 - OUT_ETHNICITY_SOFTMAX_loss: 0.8020 - OUT_GENDER_SIGMOID_loss: 0.3218 - OUT_AGE_LINEAR_MAE_AGE: 8.4267 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7093 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8582 - val_loss: 0.6520 - val_OUT_AGE_LINEAR_loss: 113.2700 - val_OUT_ETHNICITY_SOFTMAX_loss: 0.7791 - val_OUT_GENDER_SIGMOID_loss: 0.2984 - val_OUT_AGE_LINEAR_MAE_AGE: 7.7305 - val_OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7389 - val_OUT_GENDER_SIGMOID_ACC_GENDER: 0.8712 - lr: 1.0000e-04\n",
      "Epoch 23/50\n",
      "470/469 [==============================] - ETA: 0s - loss: 0.6911 - OUT_AGE_LINEAR_loss: 139.6290 - OUT_ETHNICITY_SOFTMAX_loss: 0.7813 - OUT_GENDER_SIGMOID_loss: 0.3218 - OUT_AGE_LINEAR_MAE_AGE: 8.4783 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7197 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8560\n",
      "Epoch 23: val_loss improved from 0.64109 to 0.63824, saving model to models\\1652233544\n",
      "INFO:tensorflow:Assets written to: models\\1652233544\\assets\n",
      "469/469 [==============================] - 33s 70ms/step - loss: 0.6911 - OUT_AGE_LINEAR_loss: 139.6290 - OUT_ETHNICITY_SOFTMAX_loss: 0.7813 - OUT_GENDER_SIGMOID_loss: 0.3218 - OUT_AGE_LINEAR_MAE_AGE: 8.4783 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7197 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8560 - val_loss: 0.6382 - val_OUT_AGE_LINEAR_loss: 115.9916 - val_OUT_ETHNICITY_SOFTMAX_loss: 0.7630 - val_OUT_GENDER_SIGMOID_loss: 0.2815 - val_OUT_AGE_LINEAR_MAE_AGE: 7.8320 - val_OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7439 - val_OUT_GENDER_SIGMOID_ACC_GENDER: 0.8762 - lr: 1.0000e-04\n",
      "Epoch 24/50\n",
      "470/469 [==============================] - ETA: 0s - loss: 0.6878 - OUT_AGE_LINEAR_loss: 135.6009 - OUT_ETHNICITY_SOFTMAX_loss: 0.7854 - OUT_GENDER_SIGMOID_loss: 0.3191 - OUT_AGE_LINEAR_MAE_AGE: 8.4011 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7202 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8593\n",
      "Epoch 24: val_loss did not improve from 0.63824\n",
      "469/469 [==============================] - 29s 62ms/step - loss: 0.6878 - OUT_AGE_LINEAR_loss: 135.6009 - OUT_ETHNICITY_SOFTMAX_loss: 0.7854 - OUT_GENDER_SIGMOID_loss: 0.3191 - OUT_AGE_LINEAR_MAE_AGE: 8.4011 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7202 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8593 - val_loss: 0.6435 - val_OUT_AGE_LINEAR_loss: 115.6215 - val_OUT_ETHNICITY_SOFTMAX_loss: 0.7650 - val_OUT_GENDER_SIGMOID_loss: 0.2908 - val_OUT_AGE_LINEAR_MAE_AGE: 7.8127 - val_OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7405 - val_OUT_GENDER_SIGMOID_ACC_GENDER: 0.8733 - lr: 1.0000e-04\n",
      "Epoch 25/50\n",
      "470/469 [==============================] - ETA: 0s - loss: 0.6888 - OUT_AGE_LINEAR_loss: 136.5473 - OUT_ETHNICITY_SOFTMAX_loss: 0.7906 - OUT_GENDER_SIGMOID_loss: 0.3139 - OUT_AGE_LINEAR_MAE_AGE: 8.4102 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7199 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8594\n",
      "Epoch 25: val_loss improved from 0.63824 to 0.62884, saving model to models\\1652233544\n",
      "INFO:tensorflow:Assets written to: models\\1652233544\\assets\n",
      "469/469 [==============================] - 32s 67ms/step - loss: 0.6888 - OUT_AGE_LINEAR_loss: 136.5473 - OUT_ETHNICITY_SOFTMAX_loss: 0.7906 - OUT_GENDER_SIGMOID_loss: 0.3139 - OUT_AGE_LINEAR_MAE_AGE: 8.4102 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7199 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8594 - val_loss: 0.6288 - val_OUT_AGE_LINEAR_loss: 113.6740 - val_OUT_ETHNICITY_SOFTMAX_loss: 0.7487 - val_OUT_GENDER_SIGMOID_loss: 0.2817 - val_OUT_AGE_LINEAR_MAE_AGE: 7.7162 - val_OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7410 - val_OUT_GENDER_SIGMOID_ACC_GENDER: 0.8738 - lr: 1.0000e-04\n",
      "Epoch 26/50\n",
      "470/469 [==============================] - ETA: 0s - loss: 0.6808 - OUT_AGE_LINEAR_loss: 130.9345 - OUT_ETHNICITY_SOFTMAX_loss: 0.7863 - OUT_GENDER_SIGMOID_loss: 0.3135 - OUT_AGE_LINEAR_MAE_AGE: 8.2725 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7170 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8651\n",
      "Epoch 26: val_loss did not improve from 0.62884\n",
      "469/469 [==============================] - 29s 62ms/step - loss: 0.6808 - OUT_AGE_LINEAR_loss: 130.9345 - OUT_ETHNICITY_SOFTMAX_loss: 0.7863 - OUT_GENDER_SIGMOID_loss: 0.3135 - OUT_AGE_LINEAR_MAE_AGE: 8.2725 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7170 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8651 - val_loss: 0.6396 - val_OUT_AGE_LINEAR_loss: 115.3892 - val_OUT_ETHNICITY_SOFTMAX_loss: 0.7637 - val_OUT_GENDER_SIGMOID_loss: 0.2847 - val_OUT_AGE_LINEAR_MAE_AGE: 7.8239 - val_OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7418 - val_OUT_GENDER_SIGMOID_ACC_GENDER: 0.8749 - lr: 1.0000e-04\n",
      "Epoch 27/50\n",
      "470/469 [==============================] - ETA: 0s - loss: 0.6827 - OUT_AGE_LINEAR_loss: 135.4867 - OUT_ETHNICITY_SOFTMAX_loss: 0.7814 - OUT_GENDER_SIGMOID_loss: 0.3131 - OUT_AGE_LINEAR_MAE_AGE: 8.3558 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7184 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8608\n",
      "Epoch 27: val_loss did not improve from 0.62884\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "469/469 [==============================] - 29s 62ms/step - loss: 0.6827 - OUT_AGE_LINEAR_loss: 135.4867 - OUT_ETHNICITY_SOFTMAX_loss: 0.7814 - OUT_GENDER_SIGMOID_loss: 0.3131 - OUT_AGE_LINEAR_MAE_AGE: 8.3558 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7184 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8608 - val_loss: 0.6463 - val_OUT_AGE_LINEAR_loss: 123.0257 - val_OUT_ETHNICITY_SOFTMAX_loss: 0.7652 - val_OUT_GENDER_SIGMOID_loss: 0.2815 - val_OUT_AGE_LINEAR_MAE_AGE: 8.1921 - val_OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7320 - val_OUT_GENDER_SIGMOID_ACC_GENDER: 0.8760 - lr: 1.0000e-04\n",
      "Epoch 28/50\n",
      "470/469 [==============================] - ETA: 0s - loss: 0.6668 - OUT_AGE_LINEAR_loss: 128.9292 - OUT_ETHNICITY_SOFTMAX_loss: 0.7657 - OUT_GENDER_SIGMOID_loss: 0.3100 - OUT_AGE_LINEAR_MAE_AGE: 8.1817 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7248 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8616\n",
      "Epoch 28: val_loss improved from 0.62884 to 0.62701, saving model to models\\1652233544\n",
      "INFO:tensorflow:Assets written to: models\\1652233544\\assets\n",
      "469/469 [==============================] - 31s 66ms/step - loss: 0.6668 - OUT_AGE_LINEAR_loss: 128.9292 - OUT_ETHNICITY_SOFTMAX_loss: 0.7657 - OUT_GENDER_SIGMOID_loss: 0.3100 - OUT_AGE_LINEAR_MAE_AGE: 8.1817 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7248 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8616 - val_loss: 0.6270 - val_OUT_AGE_LINEAR_loss: 109.8692 - val_OUT_ETHNICITY_SOFTMAX_loss: 0.7517 - val_OUT_GENDER_SIGMOID_loss: 0.2826 - val_OUT_AGE_LINEAR_MAE_AGE: 7.5965 - val_OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7477 - val_OUT_GENDER_SIGMOID_ACC_GENDER: 0.8738 - lr: 1.0000e-05\n",
      "Epoch 29/50\n",
      "470/469 [==============================] - ETA: 0s - loss: 0.6607 - OUT_AGE_LINEAR_loss: 128.9249 - OUT_ETHNICITY_SOFTMAX_loss: 0.7525 - OUT_GENDER_SIGMOID_loss: 0.3110 - OUT_AGE_LINEAR_MAE_AGE: 8.1829 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7343 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8675\n",
      "Epoch 29: val_loss improved from 0.62701 to 0.62695, saving model to models\\1652233544\n",
      "INFO:tensorflow:Assets written to: models\\1652233544\\assets\n",
      "469/469 [==============================] - 31s 66ms/step - loss: 0.6607 - OUT_AGE_LINEAR_loss: 128.9249 - OUT_ETHNICITY_SOFTMAX_loss: 0.7525 - OUT_GENDER_SIGMOID_loss: 0.3110 - OUT_AGE_LINEAR_MAE_AGE: 8.1829 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7343 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8675 - val_loss: 0.6270 - val_OUT_AGE_LINEAR_loss: 110.0298 - val_OUT_ETHNICITY_SOFTMAX_loss: 0.7531 - val_OUT_GENDER_SIGMOID_loss: 0.2807 - val_OUT_AGE_LINEAR_MAE_AGE: 7.5939 - val_OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7461 - val_OUT_GENDER_SIGMOID_ACC_GENDER: 0.8738 - lr: 1.0000e-05\n",
      "Epoch 30/50\n",
      "470/469 [==============================] - ETA: 0s - loss: 0.6689 - OUT_AGE_LINEAR_loss: 131.4934 - OUT_ETHNICITY_SOFTMAX_loss: 0.7613 - OUT_GENDER_SIGMOID_loss: 0.3135 - OUT_AGE_LINEAR_MAE_AGE: 8.2659 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7297 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8645\n",
      "Epoch 30: val_loss did not improve from 0.62695\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 0.6689 - OUT_AGE_LINEAR_loss: 131.4934 - OUT_ETHNICITY_SOFTMAX_loss: 0.7613 - OUT_GENDER_SIGMOID_loss: 0.3135 - OUT_AGE_LINEAR_MAE_AGE: 8.2659 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7297 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8645 - val_loss: 0.6294 - val_OUT_AGE_LINEAR_loss: 113.2015 - val_OUT_ETHNICITY_SOFTMAX_loss: 0.7505 - val_OUT_GENDER_SIGMOID_loss: 0.2819 - val_OUT_AGE_LINEAR_MAE_AGE: 7.7005 - val_OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7431 - val_OUT_GENDER_SIGMOID_ACC_GENDER: 0.8760 - lr: 1.0000e-05\n",
      "Epoch 31/50\n",
      "470/469 [==============================] - ETA: 0s - loss: 0.6660 - OUT_AGE_LINEAR_loss: 127.8434 - OUT_ETHNICITY_SOFTMAX_loss: 0.7667 - OUT_GENDER_SIGMOID_loss: 0.3096 - OUT_AGE_LINEAR_MAE_AGE: 8.1627 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7245 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8652\n",
      "Epoch 31: val_loss did not improve from 0.62695\n",
      "469/469 [==============================] - 29s 62ms/step - loss: 0.6660 - OUT_AGE_LINEAR_loss: 127.8434 - OUT_ETHNICITY_SOFTMAX_loss: 0.7667 - OUT_GENDER_SIGMOID_loss: 0.3096 - OUT_AGE_LINEAR_MAE_AGE: 8.1627 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7245 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8652 - val_loss: 0.6274 - val_OUT_AGE_LINEAR_loss: 111.4996 - val_OUT_ETHNICITY_SOFTMAX_loss: 0.7509 - val_OUT_GENDER_SIGMOID_loss: 0.2808 - val_OUT_AGE_LINEAR_MAE_AGE: 7.6264 - val_OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7439 - val_OUT_GENDER_SIGMOID_ACC_GENDER: 0.8754 - lr: 1.0000e-06\n",
      "Epoch 32/50\n",
      "470/469 [==============================] - ETA: 0s - loss: 0.6657 - OUT_AGE_LINEAR_loss: 130.1990 - OUT_ETHNICITY_SOFTMAX_loss: 0.7666 - OUT_GENDER_SIGMOID_loss: 0.3044 - OUT_AGE_LINEAR_MAE_AGE: 8.2230 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7214 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8665Restoring model weights from the end of the best epoch: 29.\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.62695\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "469/469 [==============================] - 29s 63ms/step - loss: 0.6657 - OUT_AGE_LINEAR_loss: 130.1990 - OUT_ETHNICITY_SOFTMAX_loss: 0.7666 - OUT_GENDER_SIGMOID_loss: 0.3044 - OUT_AGE_LINEAR_MAE_AGE: 8.2230 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7214 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8665 - val_loss: 0.6276 - val_OUT_AGE_LINEAR_loss: 111.4313 - val_OUT_ETHNICITY_SOFTMAX_loss: 0.7517 - val_OUT_GENDER_SIGMOID_loss: 0.2807 - val_OUT_AGE_LINEAR_MAE_AGE: 7.6233 - val_OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7445 - val_OUT_GENDER_SIGMOID_ACC_GENDER: 0.8762 - lr: 1.0000e-06\n",
      "Epoch 32: early stopping\n",
      "INFO:tensorflow:Assets written to: models/test_ethnic\\assets\n"
     ]
    }
   ],
   "source": [
    "# specify the model input with the required shape\n",
    "# (1)\n",
    "input_layer = layers.Input((HEIGHT, WIDTH, 1))\n",
    "\n",
    "# The shared layers\n",
    "# Include at least one Conv2D layer, MaxPooling2D layer and a Flatten layer\n",
    "# you can have as many layers as possible, but make sure not to overfit your model using the training data\n",
    "# (10)\n",
    "shared_layer = layers.Conv2D(\n",
    "    64, (5, 5), activation=\"relu\", padding=\"same\", name=\"IN_SHARED\"\n",
    ")(input_layer)\n",
    "shared_layer = layers.Conv2D(64, (5, 5), activation=\"relu\", padding=\"same\")(\n",
    "    shared_layer\n",
    ")\n",
    "shared_layer = layers.MaxPooling2D(2, 2)(shared_layer)\n",
    "shared_layer = layers.Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(\n",
    "    shared_layer\n",
    ")\n",
    "shared_layer = layers.Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(\n",
    "    shared_layer\n",
    ")\n",
    "shared_layer = layers.MaxPooling2D(2, 2)(shared_layer)\n",
    "shared_layer = layers.Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")(\n",
    "    shared_layer\n",
    ")\n",
    "shared_layer = layers.Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")(\n",
    "    shared_layer\n",
    ")\n",
    "shared_layer = layers.MaxPooling2D(2, 2)(shared_layer)\n",
    "shared_layer = layers.Flatten(name=\"OUT_SHARED_FLATTEN\")(shared_layer)\n",
    "\n",
    "# Task specific layers\n",
    "# Include at least one Dense layer as a task specific layer before generating the output for age\n",
    "# (2)\n",
    "age_layer = layers.Dense(256, activation=\"relu\", name=\"IN_AGE\")(shared_layer)\n",
    "age_layer = layers.Dense(128, activation=\"relu\")(age_layer)\n",
    "age_layer = layers.Dense(64, activation=\"relu\")(age_layer)\n",
    "\n",
    "# Include the age output and make sure to include the following arguments\n",
    "# activation='linear', name='xxx'(any name)\n",
    "# make sure to name your output layers so that different metrics to be used can be linked accordingly\n",
    "# please note that the age prediction is a regression task\n",
    "# (2)\n",
    "age_layer = layers.Dense(1, activation=\"linear\", name=\"OUT_AGE_LINEAR\")(age_layer)\n",
    "\n",
    "# Similar to above, specify one or more Dense layers as task specific layers for ethnicity prediction\n",
    "# (2)\n",
    "ethnicity_layer = layers.Dense(256, activation=\"relu\", name=\"IN_ETHNICITY\")(\n",
    "    shared_layer\n",
    ")\n",
    "ethnicity_layer = layers.Dense(128, activation=\"relu\")(ethnicity_layer)\n",
    "ethnicity_layer = layers.Dense(64, activation=\"relu\")(ethnicity_layer)\n",
    "# Include the ethnicity output that uses the task specific output from the layer above\n",
    "# please note that the ethnicity prediction is a multi-class classification task\n",
    "# (2)\n",
    "ethnicity_layer = layers.Dense(5, activation=\"softmax\", name=\"OUT_ETHNICITY_SOFTMAX\")(\n",
    "    ethnicity_layer\n",
    ")\n",
    "\n",
    "# Similar to above, specify one or more Dense layers as task specific layers for gender prediction\n",
    "# (2)\n",
    "gender_layer = layers.Dense(128, activation=\"relu\", name=\"IN_GENDER\")(shared_layer)\n",
    "gender_layer = layers.Dropout(0.2)(gender_layer)\n",
    "gender_layer = layers.Dense(64, activation=\"relu\")(gender_layer)\n",
    "# Include the gender output that uses the task specific output from the layer above\n",
    "# please note that the ethnicity prediction is a binary classification task\n",
    "# (2)\n",
    "gender_layer = layers.Dense(1, activation=\"sigmoid\", name=\"OUT_GENDER_SIGMOID\")(\n",
    "    gender_layer\n",
    ")\n",
    "\n",
    "# create the model with the required input and the outputs.\n",
    "# pelase make sure that the outputs can be included in a list and make sure to keep note of the order\n",
    "# (3)\n",
    "model = keras.Model(\n",
    "    inputs=input_layer, outputs=[age_layer, ethnicity_layer, gender_layer]\n",
    ")\n",
    "\n",
    "# print the model summary\n",
    "# (0.5)\n",
    "model.summary()\n",
    "\n",
    "# Instantiate the optimizer with the learning rate. You can start with the learning rate 1e-3(0.001).\n",
    "# Both the optimizer and the learning rate are hyperparameters that you can finetune\n",
    "# For example, you can start with the \"RMSprop\" optimizer\n",
    "# (2)\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "\n",
    "# specify the losses to be used for each task: age, ethnicity and gender prediction\n",
    "# (0.5)\n",
    "losses = {\n",
    "    \"OUT_AGE_LINEAR\": keras.losses.MeanSquaredError(),\n",
    "    \"OUT_ETHNICITY_SOFTMAX\": keras.losses.SparseCategoricalCrossentropy(),\n",
    "    \"OUT_GENDER_SIGMOID\": keras.losses.BinaryCrossentropy(),\n",
    "}\n",
    "\n",
    "# compile the model with the optimizer, loss, loss_weights and the metrics for each task\n",
    "# apply the following weights to the losses to balance the contribution of each loss to the total loss\n",
    "loss_weights = [0.001, 0.5, 0.5]\n",
    "\n",
    "# please remember to use the relevant metric for each task by assigning it to the correct output\n",
    "metrics = {\n",
    "    \"OUT_AGE_LINEAR\": keras.metrics.MeanAbsoluteError(name=\"MAE_AGE\"),\n",
    "    \"OUT_ETHNICITY_SOFTMAX\": keras.metrics.SparseCategoricalAccuracy(\n",
    "        name=\"ACC_ETHNICITY\"\n",
    "    ),\n",
    "    \"OUT_GENDER_SIGMOID\": keras.metrics.BinaryAccuracy(name=\"ACC_GENDER\"),\n",
    "}\n",
    "\n",
    "# (2)\n",
    "model.compile(\n",
    "    optimizer=optimizer, loss=losses, metrics=metrics, loss_weights=loss_weights\n",
    ")\n",
    "\n",
    "\n",
    "# Define the callbacks\n",
    "# EarlyStopping: monitor the validation loss while waiting for 3 epochs before stopping\n",
    "# can restore the best weights\n",
    "# (2)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=3, restore_best_weights=True, verbose=1\n",
    ")\n",
    "# ModelCheckpoint\n",
    "# monitor validation loss and save the best model weights\n",
    "# (2)\n",
    "TMP_DIR = Path(\"models\")\n",
    "checkpoints = tf.keras.callbacks.ModelCheckpoint(\n",
    "    TMP_DIR.joinpath(str(int(time.time()))),\n",
    "    verbose=1,\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True,\n",
    ")\n",
    "# Initiallize TensorBoard\n",
    "# (2)\n",
    "LOGS_DIR = Path(\"logs\")\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(LOGS_DIR.joinpath(str(int(time.time()))))\n",
    "# ReduceLROnPlateau\n",
    "# reduce the learning rate by a factor of 0.1 after waiting for 2 epochs while monitoring validation loss\n",
    "# specify a minimum learning rate to be used\n",
    "# (2)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    patience=2, verbose=1, factor=0.1, monitor=\"val_loss\", min_lr=0.000001\n",
    ")\n",
    "# fit the model with training and validation generators\n",
    "# In addition please specify the following arguments\n",
    "# steps_per_epoch=len(df_train)/batch_size\n",
    "# validation_steps=len(df_val)/batch_size\n",
    "# (5)\n",
    "CALLBACKS = [early_stop, checkpoints, tensorboard, reduce_lr]\n",
    "\n",
    "\n",
    "# NOTE: ADDED TO VISUALIZE SCHEMA FOR ME\n",
    "img_file = \"./model_arch.png\"\n",
    "tf.keras.utils.plot_model(\n",
    "    model, to_file=img_file, show_shapes=True, show_layer_names=True\n",
    ")\n",
    "\n",
    "\n",
    "# NOTE: trying to balance ethnicity data\n",
    "y_train = train_df[\"ethnicity\"].to_numpy()\n",
    "ethnicity_class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\", classes=np.unique(y_train), y=y_train\n",
    ")\n",
    "\n",
    "class_weights = {\n",
    "    \"OUT_AGE_LINEAR\": None,\n",
    "    \"OUT_ETHNICITY_SOFTMAX\": dict(enumerate(ethnicity_class_weights)),\n",
    "    \"OUT_GENDER_SIGMOID\": None,\n",
    "}\n",
    "print(\"Class weights:\", class_weights)\n",
    "\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=50,  # will stop before because of the callback 'early stop'\n",
    "    batch_size=batch_size,\n",
    "    callbacks=CALLBACKS,\n",
    "    steps_per_epoch=len(train_df) / batch_size,\n",
    "    validation_steps=len(val_df) / batch_size,\n",
    "    # NOTE: does not seem to work with multi-ouput for TF28\n",
    "    #class_weight=class_weights,\n",
    ")\n",
    "\n",
    "# NOTE: keep model to save time\n",
    "model.save(\"models/test_ethnic\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making predictions on test data (14/100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147/147 [==============================] - 19s 129ms/step - loss: 0.6143 - OUT_AGE_LINEAR_loss: 109.4439 - OUT_ETHNICITY_SOFTMAX_loss: 0.7325 - OUT_GENDER_SIGMOID_loss: 0.2773 - OUT_AGE_LINEAR_MAE_AGE: 7.5364 - OUT_ETHNICITY_SOFTMAX_ACC_ETHNICITY: 0.7419 - OUT_GENDER_SIGMOID_ACC_GENDER: 0.8829\n",
      "Ethnicity accuracy: 0.7419080138206482\n",
      "Gender accuracy: 0.8828790187835693\n"
     ]
    }
   ],
   "source": [
    "# evaluate the trained model using the test generator\n",
    "# print only the test accuracy for ethnicity and gender predictions\n",
    "# (4)\n",
    "(\n",
    "    loss,\n",
    "    age_loss,\n",
    "    ethnicity_loss,\n",
    "    gender_loss,\n",
    "    age_mae,\n",
    "    ethnicity_acc,\n",
    "    gender_acc,\n",
    ") = model.evaluate(test_generator)\n",
    "\n",
    "print(\"Ethnicity accuracy:\", ethnicity_acc)\n",
    "print(\"Gender accuracy:\", gender_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ethnicity:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.82      0.81      1991\n",
      "           1       0.77      0.83      0.80       896\n",
      "           2       0.72      0.79      0.76       683\n",
      "           3       0.66      0.65      0.66       790\n",
      "           4       0.34      0.16      0.22       336\n",
      "\n",
      "    accuracy                           0.74      4696\n",
      "   macro avg       0.66      0.65      0.65      4696\n",
      "weighted avg       0.73      0.74      0.73      4696\n",
      "\n",
      "Gender:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.91      0.89      2456\n",
      "           1       0.90      0.85      0.87      2240\n",
      "\n",
      "    accuracy                           0.88      4696\n",
      "   macro avg       0.88      0.88      0.88      4696\n",
      "weighted avg       0.88      0.88      0.88      4696\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generate predictions using the test generator\n",
    "# (2)\n",
    "predictions = model.predict(test_generator)\n",
    "\n",
    "# extract the ethnicity predictions\n",
    "# (2)\n",
    "age, ethnicity, gender = predictions\n",
    "ethnicity = np.array([np.argmax(x) for x in ethnicity])\n",
    "# print the classification report for predicting ethnicity\n",
    "# (2)\n",
    "y_true = test_df[\"ethnicity\"].to_numpy()\n",
    "classif = classification_report(y_true, ethnicity)\n",
    "print(\"ethnicity:\\n\", classif)\n",
    "\n",
    "# extract the gender predictions where probabilities above 0.5 are considered class 1 and if not, class 0\n",
    "# (2)\n",
    "gender = [x[0] for x in gender.tolist()]\n",
    "for index, g in enumerate(gender):\n",
    "    if g > 0.5:\n",
    "        gender[index] = 1\n",
    "    else:\n",
    "        gender[index] = 0\n",
    "# print the classification report for predicting gender\n",
    "# (2)\n",
    "y_true = test_df[\"gender\"].to_numpy()\n",
    "classif = classification_report(y_true, gender)\n",
    "print(\"Gender:\\n\", classif)\n",
    "\n",
    "\n",
    "# NOTE: simply for me, age\n",
    "age = [x[0] for x in age.tolist()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Present prediction results on test data(5/100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Present your findings for 5 different runs by fine-tuning the hyperparameters. The results table must contain the following fields\n",
    "\n",
    "- A minimum of 5 hyperparameters that you have fine-tuned\n",
    "- Mean absolute error for age\n",
    "- Accuracy for ethnicity prediction\n",
    "- Accuracy for gender prediction\n",
    "  Please use a table format similar to the one mentioned below when presenting the results.\n",
    "\n",
    "| Hyperparameters                                               | Age(MAE) test | Ethnicity(Accuracy) test | Gender(Accuracy) test |\n",
    "| ------------------------------------------------------------- | ------------- | ------------------------ | --------------------- |\n",
    "| initial setup: batch16, rmsprop, lr=0.001, plateau_lr=0.00001 | 7.347         | 68.75                    | 87.50                 |\n",
    "| batch32 (better)                                              | 7.599         | 74.23                    | 87.54                 |\n",
    "| batch64                                                       | 8.537         | 72.08                    | 86.67                 |\n",
    "| optimizer=Adam                                                | 9.671         | 65.20                    | 82.82                 |\n",
    "| lr=0.01                                                       | 16.85         | 43.75                    | 40.62                 |\n",
    "| lr=0.0001 (slow)                                              | 7.307         | 76.32                    | 88.46                 |\n",
    "| plateau_lr=0.000001 (better)                                  | 7.140         | 76.30                    | 88.14                 |\n",
    "\n",
    "So, I choose: batch=32, optimizer=RMSPROP, lr=0.001, plateau_lr=0.000001\n",
    "\n",
    "For the creation of the layers in the model, i did different tests (looking with tensorboard), but did not record the accuracy to show my result of this model architecture (as you can see with model_arch.png).\n",
    "\n",
    "In the shared layer, i tried different number of maxPooling and conv2d and the number of filter. I decided to do (2conv+1pool)\\*3 when increasing gradually the number of filters was the best result. Doing 1conv was underfitting and doing 3conv produce approximately the same result but with longer calculation time. Batch normalization was not necessarly because i did not overfit.\n",
    "\n",
    "In the specific layers, I decided to apply 3 dense layer with decreasing units (256-128-64) for age and ethnicity (no overfitting and no underfitting). For the gender, 3 layers was overfitting. So, I decided to reduce to 2 dense layer and 1 dropout of 0.2 in between.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "570feb405e2e27c949193ac68f46852414290d515b0ba6e5d90d076ed2284471"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
